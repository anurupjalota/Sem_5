{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMHjtVPbyaKP"
   },
   "source": [
    "## Multinomial Logistic Regression Model for Obesity Level classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EdWC89WA80ne"
   },
   "source": [
    "## Part 1.1: Implement  multinomial logistic regression using softmax from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M0agEvZVFeo5"
   },
   "source": [
    "### Logistic regression\n",
    "Logistic regression uses an equation as the representation, very much like linear regression.\n",
    "\n",
    "Input values (x) are combined linearly using weights or coefficient values (referred to as W) to predict an output value (y). A key difference from linear regression is that the output value being modeled is a binary values (0 or 1) rather than a continuous value.<br>\n",
    "\n",
    "###  $\\hat{y}(w, x) = \\frac{1}{1+exp^{-(w_0 + w_1 * x_1 + ... + w_p * x_p)}}$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Multiclass Logistic regression using Softmax\n",
    "\n",
    "The softmax function, also known as softargmax or normalized exponential function, is a generalization of the logistic function to multiple dimensions. It is used in multinomial logistic regression and is often used as the last activation function of a neural network.\n",
    "\n",
    "${\\displaystyle \\sigma (\\mathbf {z} )_{i}={\\frac {e^{z_{i}}}{\\sum _{j=1}^{K}e^{z_{j}}}}{\\text{ for }}i=1,\\dotsc ,K{\\text{ and }}\\mathbf {z} =(z_{1},\\dotsc ,z_{K})\\in \\mathbb {R} ^{K}}$\n",
    "\n",
    "Here K is the number of class and each zi is calculated using \n",
    "\n",
    "$z_i = w_0 + w_1 * x_1 + ... + w_p * x_p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJi26z8awmSD"
   },
   "source": [
    "\n",
    "#### Dataset\n",
    "The dataset is available at <strong>\"data/obesity_data.csv\"</strong> in the respective challenge's repo.<br>\n",
    "<strong>Original Source:</strong> https://archive.ics.uci.edu/ml/datasets/Estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition+\n",
    "\n",
    "This dataset include data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition. The data contains 17 attributes and 2111 records, the records are labeled with the class variable NObesity (Obesity Level), that allows classification of the data using the values of Insufficient Weight, Normal Weight, Overweight Level I, Overweight Level II, Obesity Type I, Obesity Type II and Obesity Type III. <br><br>\n",
    "\n",
    "\n",
    "#### Features (X)\n",
    "\n",
    "1. Gender {Female,Male}\n",
    "2. Age {numeric}\n",
    "3. Height {numeric}\n",
    "4. Weight {numeric}\n",
    "5. family_history_with_overweight: Has a family member suffered or suffers from overweight? {yes,no}\n",
    "6. FAVC : Do you eat high caloric food frequently? {yes,no}\n",
    "7. FCVC : Do you usually eat vegetables in your meals? {numeric}\n",
    "8. NCP : How many main meals do you have daily? {numeric}\n",
    "9. CAEC : Do you eat any food between meals? {no,Sometimes,Frequently,Always}\n",
    "10. SMOKE : Do you smoke? {yes,no}\n",
    "11. CH2O : How much water do you drink daily? {numeric}\n",
    "12. SCC : Do you monitor the calories you eat daily? {yes,no}\n",
    "13. FAF : How often do you have physical activity? {numeric}\n",
    "14. TUE : How much time do you use technological devices such as cell phone, videogames, television, computer and others? {numeric}\n",
    "15. CALC : how often do you drink alcohol? {no,Sometimes,Frequently,Always}\n",
    "16. MTRANS : Daily Transportation {Automobile,Motorbike,Bike, Public_Transportation,Walking}\n",
    "\n",
    "Take a look above at the source of the original dataset for more details.\n",
    "\n",
    "#### Target (y)\n",
    "17. NObeyesdad {Insufficient_Weight,Normal_Weight,Overweight_Level_I,Overweight_Level_II,Obesity_Type_I,Obesity_Type_II,Obesity_Type_III}\n",
    "\n",
    "#### Objective\n",
    "To gain understanding of multiclass classification using logistic regression through implementing the model from scratch\n",
    "\n",
    "#### Tasks\n",
    "- Download and load the data\n",
    "- Add intercept column with all values=1\n",
    "- Feature transformation:\n",
    "    - Convert 'Gender' column to numbers where 'Female' is 1 and 'Male' is 0\n",
    "    - Convert yes/no columns ['family_history_with_overweight','FAVC','SMOKE','SCC'] to 1/0\n",
    "    - One-Hot encode 'MTRANS', and 'NObeyesdad' columns. *Note:* One-hot encoding class/target variable is required for comparing binary predictions during training.\n",
    "    - Label encode 'CAEC', and 'CALC' columns\n",
    "    - Since the features have relatively different ranges, normalize the dataset\n",
    "- Define X matrix (independent features) and y matrix (target features) as numpy arrays\n",
    "- Print the shape and datatype of both X and y\n",
    "- Split the dataset into 80% for training and rest 20% for testing (sklearn.model_selection.train_test_split function)\n",
    "- Follow logistic regression class and fill code where highlighted:\n",
    "    - Write softmax function to predict probabilities for all classes\n",
    "    - Write cross entropy loss function\n",
    "    - Write fit function where gradient descent is implemented\n",
    "    - Write predict_proba function where we predict probabilities for input data\n",
    "    - Write predict function to select single class for given input from probabilities\n",
    "- Train the model\n",
    "- Write function for calculating accuracy\n",
    "- Compute accuracy on train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Fun (will not be evaluated)\n",
    "- Play with learning rate and max_iterations\n",
    "- Preprocess data with different feature scaling methods (i.e. scaling, normalization, standardization, etc) and observe accuracies on both X_train and X_test\n",
    "- Train model on different train-test splits such as 60-40, 50-50, 70-30, 80-20, 90-10, 95-5 etc. and observe accuracies on both X_train and X_test\n",
    "- Shuffle training samples with different random seed values in the train_test_split function. Check the model error for the testing data for each setup.\n",
    "- Print other classification metrics such as:\n",
    "    - classification report (sklearn.metrics.classification_report),\n",
    "    - confusion matrix (sklearn.metrics.confusion_matrix),\n",
    "    - precision, recall and f1 scores (sklearn.metrics.precision_recall_fscore_support)\n",
    "\n",
    "#### Helpful links\n",
    "- Multiclass Logistic Regression from scratch: https://gluon.mxnet.io/chapter02_supervised-learning/softmax-regression-scratch.html\n",
    "- Softmax tutorial : http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/\n",
    "- Softmax function detailed history: https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d\n",
    "- Understand gradients for cross entropy loss: https://rstudio-pubs-static.s3.amazonaws.com/337306_79a7966fad184532ab3ad66b322fe96e.html\n",
    "- OnevsRest (OVR) strategy for multiclass classification: https://medium.com/analytics-vidhya/logistic-regression-from-scratch-multi-classification-with-onevsall-d5c2acf0c37c\n",
    "- Use slack for doubts: https://join.slack.com/t/deepconnectai/shared_invite/zt-givlfnf6-~cn3SQ43k0BGDrG9_YOn4g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21J6cpd_wmSE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4SL1fdNt1k3Q"
   },
   "outputs": [],
   "source": [
    "# Download the dataset from the source\n",
    "!wget https://github.com/DeepConnectAI/challenge-week-4/raw/master/data/obesity_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9av7W-wowmSI"
   },
   "outputs": [],
   "source": [
    "# Read the data from local cloud directory\n",
    "data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOtKXuWL80n4"
   },
   "outputs": [],
   "source": [
    "# Add the intercept column with all values 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eV1jGAQxwmSP"
   },
   "outputs": [],
   "source": [
    "# Print some rows just to understand data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature transformation as per the above tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "joRU6dWxwmSR"
   },
   "outputs": [],
   "source": [
    "# Define X (input features) and y (output feature) \n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAyM-CYCwmSU"
   },
   "outputs": [],
   "source": [
    "X_shape = \n",
    "X_type  = \n",
    "y_shape = \n",
    "y_type  = \n",
    "print(f'X: Type-{X_type}, Shape-{X_shape}')\n",
    "print(f'y: Type-{y_type}, Shape-{y_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6W4_2vt80oI"
   },
   "source": [
    "Shape of X will depend on how feature transformation is done, but number of columns should be >=17, and there should be 2111 rows.\n",
    "\n",
    "Nevertheless, type of X must be <class 'numpy.ndarray'>\n",
    "\n",
    "<strong>Output for y: </strong><br>\n",
    "\n",
    "y: Type-<class 'numpy.ndarray'>, Shape-(2111,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8WF-EqO3BEa"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing here\n",
    "# Use RNADOM STATE parameter as well to reproduce results later\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "acCATJhI3FdH"
   },
   "outputs": [],
   "source": [
    "# Print the shape of features and target of training and testing: X_train, X_test, y_train, y_test\n",
    "X_train_shape = \n",
    "y_train_shape = \n",
    "X_test_shape  = \n",
    "y_test_shape  = \n",
    "\n",
    "print(f\"X_train: {X_train_shape} , y_train: {y_train_shape}\")\n",
    "print(f\"X_test: {X_test_shape} , y_test: {y_test_shape}\")\n",
    "assert (X_train.shape[0]==y_train.shape[0] and X_test.shape[0]==y_test.shape[0]), \"Check your splitting carefully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eSa7cW-NwmSd"
   },
   "source": [
    "##### Let us start implementing logistic regression from scratch. Just follow code cells, see hints if required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tuy_qkR280oY"
   },
   "source": [
    "##### We will build a LogisticRegression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23RZbK3P80oZ"
   },
   "outputs": [],
   "source": [
    "# DO NOT EDIT ANY VARIABLE OR FUNCTION NAME(S) IN THIS CELL\n",
    "# Let's try more object oriented approach this time :)\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        '''Initialize variables\n",
    "        Args:\n",
    "            learning_rate  : Learning Rate\n",
    "            max_iterations : Max iterations for training weights\n",
    "        '''\n",
    "        # Initialising all the parameters\n",
    "        self.learning_rate  = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.losses    = []\n",
    "        \n",
    "        # Define epsilon because log(0) is not defined\n",
    "        self.eps = 1e-7\n",
    "\n",
    "    def softmax(self, z):\n",
    "        '''Softmax function\n",
    "        Args:\n",
    "            z : A numpy array (num_samples,num_classes)\n",
    "        Returns:\n",
    "            A numpy array where softmax function applied to every sample\n",
    "        '''\n",
    "        assert len(z.shape) == 2\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        soft_z = \n",
    "        ### END CODE HERE\n",
    "        \n",
    "        return soft_z\n",
    "    \n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        '''Compute cross_entropy_loss\n",
    "        Args:\n",
    "            y_true : Numpy array of actual truth values (num_samples,num_classes)\n",
    "            y_pred : Numpy array of predicted values (num_samples,num_classes)\n",
    "        Returns:\n",
    "            Cross-entropy loss, scalar value (sum of cross entropy loss of individual classes)\n",
    "        '''\n",
    "        # Fix 0/1 values in y_pred so that log is not undefined\n",
    "        y_pred = np.maximum(np.full(y_pred.shape, self.eps), np.minimum(np.full(y_pred.shape, 1-self.eps), y_pred))\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        # HINT: Take sum of losses of all classes\n",
    "        ce_loss = \n",
    "        ### END CODE HERE\n",
    "        \n",
    "        return ce_loss\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''Trains logistic regression model using gradient ascent\n",
    "        to gain maximum likelihood on the training data\n",
    "        Args:\n",
    "            X : Numpy array (num_examples, num_features)\n",
    "            y : Numpy array (num_examples, num_classes)\n",
    "        Returns: VOID\n",
    "        '''\n",
    "        \n",
    "        num_examples = X.shape[0]\n",
    "        num_features = X.shape[1]\n",
    "        num_classes  = y.shape[1]\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        \n",
    "        # Initialize weights with appropriate shape [num_features, num_classes]\n",
    "        self.weights = \n",
    "        \n",
    "        # Perform gradient ascent\n",
    "        for i in range(self.max_iterations):\n",
    "            # Define the linear hypothesis(z) first\n",
    "            z = \n",
    "            \n",
    "            # Output probability values using softmax\n",
    "            y_pred = \n",
    "            \n",
    "            # Compute gradient for weights assiciated with each class \n",
    "            gradient = np.dot(X.T, (y_pred -  y))\n",
    "            \n",
    "            # Update the weights\n",
    "            # Perform weight updation for each class\n",
    "            self.weights = \n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = \n",
    "\n",
    "            self.losses.append(loss)\n",
    "    \n",
    "        ### END CODE HERE\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        '''Predict probabilities for given X.\n",
    "        Remember sigmoid returns value between 0 and 1.\n",
    "        Args:\n",
    "            X : Numpy array (num_samples, num_features)\n",
    "        Returns:\n",
    "            probabilities: Numpy array (num_samples,num_classes)\n",
    "        '''\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"Fit the model before prediction\")\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        z = \n",
    "        probabilities = \n",
    "        ### END CODE HERE\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Predict/Classify X in classes\n",
    "        Args:\n",
    "            X         : Numpy array (num_samples, num_features)\n",
    "        Returns:\n",
    "            binary_predictions : Numpy array (num_samples, num_classes)\n",
    "        '''\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        # HINT: Choose maximum probability (Set value to 1 and rest 0) to predict binary values using np.argmax() function\n",
    "        \n",
    "        binary_predictions = \n",
    "        \n",
    "        ### END CODE HERE\n",
    "        \n",
    "        return binary_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FpOjju_480oc"
   },
   "outputs": [],
   "source": [
    "# Now initialize multinomial logitic regression implemented by you\n",
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3hevcFF80oe"
   },
   "outputs": [],
   "source": [
    "# And now fit on training data\n",
    "model.fit(?, ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8oyz8ev780oj"
   },
   "source": [
    "##### Phew!! That's a lot of code. But you did it, congrats !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tvMc0OqwmSp"
   },
   "outputs": [],
   "source": [
    "# Train log-likelihood\n",
    "train_loss = model.cross_entropy_loss(y_train, model.predict_proba(X_train))\n",
    "print(\"Loss on training data:\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZQ8ITUt4b0N"
   },
   "outputs": [],
   "source": [
    "# Test log-likelihood\n",
    "test_loss = model.cross_entropy_loss(y_test, model.predict_proba(X_test))\n",
    "print(\"Loss on testing data:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPZ6w2IK80oq"
   },
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot([i+1 for i in range(len(model.losses))], model.losses)\n",
    "plt.title(\"Loss curve\")\n",
    "plt.xlabel(\"Iteration num\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_PXpSyC80ov"
   },
   "source": [
    "##### Let's calculate accuracy as well. Accuracy is defined simply as the rate of correct classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iDow5HNv80oy"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_true,y_pred):\n",
    "    '''Compute accuracy.\n",
    "    Accuracy = (Correct prediction / number of samples)\n",
    "    Args:\n",
    "        y_true : Truth binary values (num_examples, num_classes)\n",
    "        y_pred : Predicted binary values (num_examples, num_classes)\n",
    "    Returns:\n",
    "        accuracy: scalar value\n",
    "    '''\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    \n",
    "    accuracy = \n",
    "    ### END CODE HERE\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPlTLEDw80o2"
   },
   "outputs": [],
   "source": [
    "# Print accuracy on train data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_dM97OsR80o4"
   },
   "outputs": [],
   "source": [
    "# Print accuracy on test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otuidf-v80o7"
   },
   "source": [
    "## Part 1.2: Use Logistic Regression from sklearn on the same dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8lecvVzu80o7"
   },
   "source": [
    "#### Tasks\n",
    "- Define X and y again for sklearn Linear Regression model\n",
    "- <strong>Note:</strong>\n",
    "    - Column at position 0 with all values=1 is not required. (Handled by scikit-learn built-in class)\n",
    "    - One-hot encoding of the target column is not required. (Handled by scikit-learn built-in class)\n",
    "    - Don't scale/normalize the target column, let them be whole numbers 0,1,2,... (sklearn does not recognize continuous values as categories)\n",
    "- Train Logistic Regression Model on the training set (sklearn.linear_model.LogisticRegression class)\n",
    "- Run the model on testing set\n",
    "- Print 'accuracy' obtained on the testing dataset (sklearn.metrics.accuracy_score function)\n",
    "\n",
    "#### Further fun (will not be evaluated)\n",
    "- Compare accuracies of your model and sklearn's logistic regression model\n",
    "\n",
    "#### Helpful links\n",
    "- Classification metrics in sklearn: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "- Feature Scaling: https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-T6e7_o80o8"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iHqyjdEsL3Yv"
   },
   "outputs": [],
   "source": [
    "# Perform data loading and preprocessing here suitable for sklearn model\n",
    "# See above note in tasks to minimize implementation errors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oc4hFVS_80o-"
   },
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X_s = \n",
    "y_s = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the SAME TEST SIZE AND RANDOM STATE as above splitting to compare right\n",
    "X_s_train, X_s_test, y_s_train, y_s_test = train_test_split(X_s, y_s, test_size=0.20, random_state=?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrXCCTOQ80pF"
   },
   "outputs": [],
   "source": [
    "# Initialize the model from sklearn\n",
    "sk_model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x7Xa-l7m80qC"
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "sk_model.fit(X_s_train, y_s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rCQvgSp80qH"
   },
   "outputs": [],
   "source": [
    "# Predict on testing set X_test\n",
    "y_s_pred = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4K1rMC-80qL"
   },
   "outputs": [],
   "source": [
    "# Print Accuracy on testing set\n",
    "test_accuracy_sklearn = \n",
    "\n",
    "print(f\"\\nAccuracy on testing set: {test_accuracy_sklearn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUG9Wk_880qN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task_1_multiclass_logistic_obesity.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
